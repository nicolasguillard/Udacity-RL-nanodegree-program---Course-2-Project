{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent DQN with Experience replay and Fixed Q-target\n",
    "\n",
    "- Created : **22/03/2025**\n",
    "- Updated : **15/05/2025**\n",
    "\n",
    "PoC using DQN from exercice 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in and 3rd party modules\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "# Custom modules\n",
    "from dqn_agent__exp_replay import DQNAgentExpReplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "print(f\"Availabe device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode_Stats = namedtuple(\"Experience\", field_names=[\"i_episode\", \"steps_to_resolution\", \"score\", \"is_solution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_stats(\n",
    "        episode_stats, mean_scores, shift_score_avgs, solution_threshold, scores_window_length, title=\"\"):\n",
    "    # plot the scores\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    scores = [episode.score for episode in episode_stats]\n",
    "    ax.plot(np.arange(1, len(scores)+1), scores, color='b', label='Scores')\n",
    "    ax.plot(\n",
    "        np.arange(1, len(mean_scores)+1),\n",
    "        mean_scores,\n",
    "        color='y',\n",
    "        linestyle='--',\n",
    "        label='Score average from start'\n",
    "        )\n",
    "    ax.plot(\n",
    "        np.arange(scores_window_length, scores_window_length + len(shift_score_avgs)),\n",
    "        shift_score_avgs,\n",
    "        color='r',\n",
    "        label=f\"{scores_window_length} episodes shift average\"\n",
    "        )\n",
    "    ax.set_ylabel('Score')\n",
    "    \n",
    "    x_scatter = []\n",
    "    y_scatter = []\n",
    "    max_steps_to_resolution = 0\n",
    "    for episode in episode_stats:\n",
    "        if episode.is_solution:\n",
    "            x_scatter.append(episode.i_episode)\n",
    "            y_scatter.append(episode.steps_to_resolution)\n",
    "            max_steps_to_resolution = max(max_steps_to_resolution, episode.steps_to_resolution)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.scatter(\n",
    "        x_scatter,\n",
    "        y_scatter,\n",
    "        s=2,\n",
    "        color='g',\n",
    "        hatch=\"x\",\n",
    "        label='Steps to solution'\n",
    "    )\n",
    "    ax2.set_ylim(0, int(1.1 * max_steps_to_resolution))\n",
    "    ax2.set_ylabel('Steps')\n",
    "\n",
    "    ax.axhline(y=solution_threshold, color='k', linestyle='--', label='Solution threshold')\n",
    "    \n",
    "    ax.set_xlabel('Episode #')\n",
    "    plt.title(title)\n",
    "    fig.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_agent(\n",
    "        agent,\n",
    "        env,\n",
    "        brain_name,\n",
    "        eps_scheduler,\n",
    "        eps_start=1.0,\n",
    "        n_episodes=2000,\n",
    "        solution_threshold=13,\n",
    "        avg_window_length_scores=100,\n",
    "        print_stats_each_n_episode=25,\n",
    "        window_stability=1,\n",
    "        max_timesteps=300\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Deep Q-Learning agent training function.\n",
    "    \n",
    "    Args :\n",
    "        - n_episodes (int): maximum number of training episodes\n",
    "        - max_timesteps (int): maximum number of timesteps per episode\n",
    "        - eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        - eps_end (float): minimum value of epsilon\n",
    "        - eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []             # list containing scores from each episode\n",
    "    score_avgs = []\n",
    "    shift_score_avgs = []\n",
    "    stats_episodes = []\n",
    "\n",
    "    eps = eps_start\n",
    "    solved = False\n",
    "    last_solved = 0\n",
    "    solved_episode_count = 0\n",
    "\n",
    "    agent.train()\n",
    "    for i_episode in trange(1, n_episodes+1, desc=\"Training\", unit=\"episode\", leave=False):\n",
    "        # Reset the environment\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        # Initial state\n",
    "        state = env_info.vector_observations[0]\n",
    "\n",
    "        score = 0               # episode score\n",
    "        actions_of_episode = [] # list of actions taken during the episode\n",
    "        steps_to_resolution = 0\n",
    "        # Loop for each episode\n",
    "        for i_step in range(1, max_timesteps+1):\n",
    "            # Agent chooses action\n",
    "            action = agent.act(state, eps=eps)\n",
    "            actions_of_episode.append(action)\n",
    "            \n",
    "            # Apply action to environment and get environment evolution as\n",
    "            # experience : next state, reward and done\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0] \n",
    "            \n",
    "            score += reward\n",
    "            if score >= solution_threshold and steps_to_resolution == 0:\n",
    "                steps_to_resolution = i_step\n",
    "            \n",
    "            # Agent learns from the experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state                \n",
    "            \n",
    "            # Check if episode is done (a.k.a terminal state)\n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        # Save episode stats\n",
    "        stats_episodes.append(\n",
    "            Episode_Stats(i_episode, steps_to_resolution, score, score >= solution_threshold)\n",
    "            )\n",
    "        # Save episode score\n",
    "        scores.append(score)\n",
    "        score_avgs.append(np.mean(scores))\n",
    "        \n",
    "        if score >= solution_threshold:\n",
    "            last_solved = i_episode\n",
    "            solved_episode_count += 1\n",
    "\n",
    "        # Process mean of scores when enough scores\n",
    "        if i_episode >= avg_window_length_scores:\n",
    "            shift_score_avgs.append(np.mean(scores[-avg_window_length_scores:]))\n",
    "\n",
    "            # Print stats regularly\n",
    "            if i_episode % print_stats_each_n_episode == 0:\n",
    "                print(f'\\rEpisode {i_episode} | Mean scores {score_avgs[-1]:.2f}' + \\\n",
    "                        f'| {avg_window_length_scores} shift score average: {shift_score_avgs[-1]:.2f}' + \\\n",
    "                        f'| Environment solved {solved_episode_count} time(s).',\n",
    "                        end=\" \")\n",
    "                if last_solved > 0:\n",
    "                    print(f'Last solution at episode {last_solved}th with score {stats_episodes[-1].score}.', end=\"\")\n",
    "\n",
    "            # Check the stability of the solution\n",
    "            if np.mean(shift_score_avgs[-window_stability:]) >= solution_threshold:\n",
    "                print(f'\\n>> Environment solved in {i_episode} episodes!\\tAverage Score: {shift_score_avgs[-1]:.2f}')\n",
    "                agent.save('checkpoint_solved.pth')\n",
    "                solved = True\n",
    "                break\n",
    "\n",
    "        # Update epsilon\n",
    "        eps = eps_scheduler(i_step, eps)\n",
    "\n",
    "    # Save the last episode if environment not solved\n",
    "    if not solved:\n",
    "        agent.save('checkpoint_last_episode.pth')\n",
    "\n",
    "    return stats_episodes, score_avgs, shift_score_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = None #71\n",
    "\n",
    "if SEED is not None:\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent neural netwotk settings\n",
    "model_parameters = {\n",
    "    \"fc1_units\": 64,\n",
    "    \"fc2_units\": 64,\n",
    "}\n",
    "\n",
    "# Training and test hyperparameters\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.995\n",
    "\n",
    "n_episodes_train = 2000\n",
    "max_timesteps = 300 # max number of timesteps per episode then done, resulting from the \"Take Random Actions in the Environment\" test\n",
    "window_stability = 2\n",
    "avg_window_length_scores = 100 # Project success constraint\n",
    "solution_threshold = 13 # Project success constraint\n",
    "print_stats_each_n_episode = 25\n",
    "\n",
    "n_episodes_test = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the agent\n",
    "agent = DQNAgentExpReplay (\n",
    "    state_size=brain.vector_observation_space_size,\n",
    "    action_size=brain.vector_action_space_size,\n",
    "    model_parameters=model_parameters,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# eps_scheduler : function to modify epsilon\n",
    "# i_episode (based on 1) is the number of steps\n",
    "# eps is the current epsilon\n",
    "eps_scheduler = lambda i_episode, eps, : max(eps_end, eps_decay*eps)\n",
    "\n",
    "stats_episodes, score_avgs, shift_score_avgs = train_dqn_agent(\n",
    "    agent,\n",
    "    env,\n",
    "    brain_name,\n",
    "    eps_scheduler,\n",
    "    eps_start=1.0,\n",
    "    n_episodes=n_episodes_train,\n",
    "    avg_window_length_scores=avg_window_length_scores,\n",
    "    solution_threshold=solution_threshold,\n",
    "    window_stability=window_stability,\n",
    "    print_stats_each_n_episode=print_stats_each_n_episode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_stats(\n",
    "    stats_episodes, score_avgs, shift_score_avgs, solution_threshold, avg_window_length_scores,\n",
    "    \"Training : scores and score averages\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload = True\n",
    "if reload:\n",
    "    agent = DQNAgentExpReplay(\n",
    "        state_size=brain.vector_observation_space_size,\n",
    "        action_size=brain.vector_action_space_size,\n",
    "        model_parameters=model_parameters,\n",
    "        device=device\n",
    "        )\n",
    "    agent.load('checkpoint_solved.pth')\n",
    "agent.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every_n_episodes = 1\n",
    "\n",
    "success = False\n",
    "score_avg = 0\n",
    "\n",
    "scores = []\n",
    "stats_episodes = []\n",
    "score_avgs = []\n",
    "shift_score_avgs = []\n",
    "for i_episode in trange(1, n_episodes_test+1):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    \n",
    "    score = 0    \n",
    "    done = False\n",
    "    steps_to_resolution = 0\n",
    "    while not done:\n",
    "        # Agent chooses action\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Apply action to environment\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        \n",
    "        # Get environment evolution as experience : next state, reward and done\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        state = next_state\n",
    "\n",
    "         # Update the score\n",
    "        score += reward                               \n",
    "        steps_to_resolution += 1\n",
    "\n",
    "        print(f\"\\rEpisode #{i_episode} : Score = {int(score)} in {steps_to_resolution} steps| Score avg = {score_avg:.2f}\", end=\"\")\n",
    "\n",
    "    # Save episode stats\n",
    "    stats_episodes.append(\n",
    "            Episode_Stats(i_episode, steps_to_resolution, score, score >= solution_threshold)\n",
    "            )\n",
    "    scores.append(score)\n",
    "    score_avg = np.mean(scores)\n",
    "    score_avgs.append(score_avg)\n",
    "    \n",
    "    if i_episode >= avg_window_length_scores:\n",
    "        shift_score_avgs.append(np.mean(scores[-avg_window_length_scores:]))\n",
    "        if np.mean(shift_score_avgs[-window_stability:]) >= solution_threshold:\n",
    "            print(f\"\\rEpisode #{i_episode} : Score = {int(score)} in {steps_to_resolution} steps| Score avg = {score_avgs[-1]:.2f}\", end=\"\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_stats(\n",
    "    stats_episodes, score_avgs, shift_score_avgs, solution_threshold, avg_window_length_scores,\n",
    "    \"Testing : scores and score averages\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End closing Unity env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

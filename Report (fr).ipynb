{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f240c606",
   "metadata": {},
   "source": [
    "# Rapport du projet Navigation - Cours 2 \"Basé sur la fonction de valeur\" - Programme nanodegré \"Apprentissage profond par renforcement\" - Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6ede3",
   "metadata": {},
   "source": [
    "# L'objectif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434a967",
   "metadata": {},
   "source": [
    "Dans l'environnement Unity \"Banana\" fourni, il s'agit d'entrainer un agent à récolter les bananes jaunes, tout en évitant les bananes bleues, durant un épisode de 300 pas maximum (imposé par l'environnement), afin d'atteindre un score supérieur ou égal à 13. L'objectif consiste à obtenir une moyenne de score glissante sur 100 épisodes supérieure ou égale à 13. Un objectif optionnel consiste à entrainer l'agent à atteindre cet objectif en moins de 1800 épisodes.\n",
    "\n",
    "Pour cela, l'agent reposera sur une méthode d'apprentissage par renforcement, Deep Q-Network (DQN), basée sur la fonction de valeur qui est estimée par un réseau de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a7a67",
   "metadata": {},
   "source": [
    "## La méthode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7573d97",
   "metadata": {},
   "source": [
    "Cet algorithme DQN est implémenté en python avec la librairie [PyTorch](https://pytorch.org/). Il s'agit d'un algorithme d'apprentissage par renforcement qui utilise un réseau de neurones pour approximer la fonction de valeur d'action. L'agent apprend à partir des récompenses qu'il reçoit en interagissant avec l'environnement, en ajustant ses actions pour maximiser la récompense cumulative à long terme.\n",
    "\n",
    "Le réseau de neurones utilisé comme estimateur introduit une instabilité dans l'apprentissage. Pour remédier à cela, nous utilisons un replay memory pour stocker les expériences passées et un réseau cible pour réduire cette instabilité. Le réseau cible est mis à jour moins fréquemment que le réseau principal, ce qui permet de réduire la variance des mises à jour.\n",
    "\n",
    "Comme l'utilisation de l'algorithme DQN nécessite un grand nombre d'itérations pour converger vers une politique optimale, il est essentiel de mettre en place des mécanismes d'exploration et d'exploitation. L'agent doit explorer différentes actions pour découvrir celles qui mènent à des récompenses élevées, tout en exploitant les connaissances acquises pour maximiser la récompense à chaque étape.\n",
    "\n",
    "La boucle d'apprentissage repose sur une mise à jour régulière des poids du réseau de neurones, en utilisant la fonction de perte entre la valeur d'action estimée et la valeur d'action cible. La fonction de perte utilisée est l'erreur quadratique moyenne (MSE), qui mesure la différence entre les valeurs prédites par le réseau et les valeurs cibles.\n",
    "\n",
    "Entre chaque mise à jour, l'agent interagit un nombre déterminé de fois avec l'environnement en choisissant des actions basées sur une stratégie $\\epsilon$-greedy. Cela signifie qu'il choisit une action aléatoire avec une probabilité $\\epsilon$, et la meilleure action connue avec une probabilité $1 - \\epsilon$. Cette stratégie permet à l'agent d'explorer de nouvelles actions tout en exploitant les connaissances acquises.\n",
    "\n",
    "Ces interactions sont stockées dans le replay memory, qui est un tampon circulaire de taille fixe. Lors de chaque mise à jour, un échantillon aléatoire d'expériences est extrait du replay memory pour entraîner le réseau de neurones sur un batch d'interactions. Cela permet de briser la corrélation entre les expériences successives et d'améliorer la stabilité de l'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683eb233",
   "metadata": {},
   "source": [
    "Le réseau de neurones utilisé dans l'algorithme DQN est un réseau linéaire qui prend pour entrée l'état de l'environnement et produit en sortie les valeurs d'action pour chaque action possible. La taille de l'entrée du réseau correspond à la taille de l'espace d'état de l'environnement, et la taille de la sortie correspond au nombre d'actions possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e680dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mono path[0] = '/Users/me/Dropbox (Compte personnel)/Perso NG/Cours et Mooc/Udacity/Deep Reinforcement learning/Cours 2 - Value-Based Methods/Udacity Course 2 Project/Udacity Course 2 Project - Source/Banana.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/me/Dropbox (Compte personnel)/Perso NG/Cours et Mooc/Udacity/Deep Reinforcement learning/Cours 2 - Value-Based Methods/Udacity Course 2 Project/Udacity Course 2 Project - Source/Banana.app/Contents/MonoBleedingEdge/etc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea0ad9",
   "metadata": {},
   "source": [
    "Voici un aperçu de l'architecture du réseau de neurones utilisé dans l'algorithme DQN, par rapport à l'environnement \"Banana\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765e9225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetworkLinear(\n",
      "  (fc1): Linear(in_features=37, out_features=64, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.linear_v1 import QNetworkLinear\n",
    "\n",
    "model = QNetworkLinear(\n",
    "    state_size=brain.vector_observation_space_size,\n",
    "    action_size=brain.vector_action_space_size,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa8f92",
   "metadata": {},
   "source": [
    "## Les résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c5f50",
   "metadata": {},
   "source": [
    "### L'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818f829",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Nous obtenons les résultats suivants en exécutant la partie entrainement du carnet `Training DQN Agent.ipynb` sur un MacbookAir M2 :\n",
    "\n",
    "<img src=\"./images/model_weights_275_solved training stats.png\" alt=\"Stats from training with Training DQN Agent.ipynb notebook\" width=\"600\"/>\n",
    "\n",
    "- L'apprentissage de l'agent permet de résoudre l'environnement en 521 épisodes (bien en dessous des 1800 épisodes indiqués pour le défi), avec une contrainte de nombre de pas de temps par épisode de 250 (au lieu des 300 imposés par l'environnement).\n",
    "- On constate que l'agent résoud de plus en plus rapidement l'environnement au fur et à mesure de l'entrainement, comme l'indique la courbe de moyenne des pas de temps pour réussir à résoudre l'environnement (c'est à dire atteindre le score de 13).\n",
    "\n",
    "Les poids du modèle associé à l'agent sont sauvegardés dans le fichier `model_weights_275_solved.pth` et peuvent être utilisés pour tester l'agent dans l'environnement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8656055",
   "metadata": {},
   "source": [
    "### L'évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c971fc",
   "metadata": {},
   "source": [
    "Nous obtenons les résultats suivants en exécutant la partie évaluation du carnet `Training DQN Agent.ipynb`:\n",
    "\n",
    "<img src=\"./images/model_weights_275_solved evaluation stats.png\" alt=\"Stats from training with Training DQN Agent.ipynb notebook\" width=\"600\"/>\n",
    "\n",
    "Nous constatons que les objectifs fixés ont été atteints, même avec des contraintes plus fortes sur le nombre d'épisodes pour résoudre le problème, le nombre maximum de pas de temps disponibles par épisode, ainsi qu'avec la stabilité de la moyenne glissante sur 100 épisodes. Nous remarquons aussi que bien qu'après 100 épisodes la moyenne glissante est stable. En fin, seul un épisode réussi ne l'est pas en respectant la contrainte du nombre maximum de pas de temps imposé.\n",
    "\n",
    "En revanche, une proportion non négligeable d'épisodes (~29%) ne sont pas résolus, mais ils sont compensés par les scores importants de ceux réussis.\n",
    "\n",
    "Une vidéo de l'interaction de l'agent avec l'environnement est disponible sur [YouTube](https://youtu.be/G3rj4Yoc8bQ).\n",
    "\n",
    "<img src=\"./images/youtube video.png\" alt=\"Youtube video\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7844e1a8",
   "metadata": {},
   "source": [
    "## Idées de travail futur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1e6ad",
   "metadata": {},
   "source": [
    "Quelques idées pour améliorer la méthode exploitant DQN :\n",
    "- effectuer des recherches d'optimisation de valeurs des hyperparamètres liés à la structure du réseau de neurones (nombre de couches, taille des couches, etc.) et à l'algorithme DQN (taux d'apprentissage, taille du batch, le facteur de discount $\\gamma$, le taux d'exploration $\\epsilon$, le nombre de pas de temps par épisode, etc.)\n",
    "- étudier l'importance de l'initialisation du réseau\n",
    "- utilisation d'autres extensions de DQN, comme Double DQN, Dueling DQN, Prioritized Experience Replay, etc.\n",
    "- étudier l'évolution d'obtention des récompenses négatives :\n",
    "    - ajouter des pénalités pmlus importantes ?\n",
    "    - comment augmenter des gains potentiels à moyen terme entrainer des actions produisant des récompenses négatives à court terme (en jouant sur $\\gamma$ et la longueur d'une séquence)\n",
    "- passer en mode d'apprentissage avec l'espace d'état pixel pour constater si l'agent peut être plus performant, et quel coût cela engendre par rapport à l'apprentissage avec l'espace d'état \"discret\".\n",
    "\n",
    "Autres idées de constats pouvant fournir des pistes d'amélioration :\n",
    "- étudier sur le taux d'échec des épisodes, alors que la moyenne des récompenses atteint l'objectif.\n",
    "- combien d'épisodes d'entrainement est-il nécessaire pour augmenter le taux d'épisodes réussis en évaluation ?\n",
    "- quelle moyenne de score maximale peut-on atteindre ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a908406",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f240c606",
   "metadata": {},
   "source": [
    "# Navigation project report - Course 2 “Value-Based Methods” - Nanodegree program “Deep Reinforcement Learning” - Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6ede3",
   "metadata": {},
   "source": [
    "# The purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434a967",
   "metadata": {},
   "source": [
    "In the provided Unity “Banana” environment, the aim is to train an agent to harvest yellow bananas, while avoiding blue bananas, during an episode of 300 timesteps maximum (imposed by the environment), in order to achieve a score greater than or equal to 13. The objective is to obtain a shifted average score over 100 episodes greater than or equal to 13. An optional objective is to train the agent to reach this objective in less than 1800 episodes.\n",
    "\n",
    "For this, the agent will rely on a reinforcement learning method Deep Q-Network (DQN), based on the value function, which is estimated by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a7a67",
   "metadata": {},
   "source": [
    "## The method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7573d97",
   "metadata": {},
   "source": [
    "The DQN method is implemented in python with the [PyTorch](https://pytorch.org/) library. It is a method that uses a neural network to approximate the action value function. The agent learns from the rewards it receives by interacting with the environment, adjusting its actions to maximize the long-term cumulative reward.\n",
    "\n",
    "The neural network used as an estimator introduces instability into the learning process. To remedy this, we use a replay memory (**experience replay**) to store past experiences and a target network (**fixed q-target**) to reduce this instability. The target network is updated less frequently than the main network, thus reducing the variance of updates.\n",
    "\n",
    "Since using the DQN algorithm requires a large number of iterations to converge on an optimal policy, it is essential to set up exploration and exploitation mechanisms. The agent must explore different actions to discover those that lead to high rewards, while exploiting the knowledge acquired to maximize the reward at each stage.\n",
    "\n",
    "The learning loop relies on regular updating of the neural network weights, using the loss function between the estimated action value and the target action value. The loss function used is the mean square error (MSE), which measures the difference between the values predicted by the network and the target values.\n",
    "\n",
    "Between each update, the agent interacts a set number of times with the environment, choosing actions based on a $\\epsilon$-greedy strategy. This means it chooses a random action with probability $\\epsilon$, and the best known action with probability $1 - \\epsilon$. This strategy allows the agent to explore new actions while exploiting the knowledge it has acquired.\n",
    "\n",
    "These interactions are stored in the replay memory, which is a circular buffer of fixed size. At each update, a random sample of experiences is extracted from the replay memory to train the neural network on a batch of interactions. This breaks the correlation between successive experiments and improves learning stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683eb233",
   "metadata": {},
   "source": [
    "The neural network used in the DQN algorithm is a linear network which takes as input the state of the environment and produces as output the action values for each possible action. The size of the input of the network corresponds to the size of the state space of the environment, and the size of the output corresponds to the number of possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e680dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mono path[0] = '/Users/me/Dropbox (Compte personnel)/Perso NG/Cours et Mooc/Udacity/Deep Reinforcement learning/Cours 2 - Value-Based Methods/Udacity Course 2 Project/Udacity Course 2 Project - Source/Banana.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/me/Dropbox (Compte personnel)/Perso NG/Cours et Mooc/Udacity/Deep Reinforcement learning/Cours 2 - Value-Based Methods/Udacity Course 2 Project/Udacity Course 2 Project - Source/Banana.app/Contents/MonoBleedingEdge/etc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea0ad9",
   "metadata": {},
   "source": [
    "Here's an overview of the neural network architecture used in the DQN algorithm, in relation to the “Banana” environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765e9225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetworkLinear(\n",
      "  (fc1): Linear(in_features=37, out_features=64, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.linear_v1 import QNetworkLinear\n",
    "\n",
    "model = QNetworkLinear(\n",
    "    state_size=brain.vector_observation_space_size,\n",
    "    action_size=brain.vector_action_space_size,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa8f92",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c5f50",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818f829",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We obtain the following results running the training part of the `Training DQN Agent.ipynb` notebook, on a MacbookAir M2:\n",
    "\n",
    "<img src=\"./images/model_weights_275_solved training stats.png\" alt=\"Stats from training with Training DQN Agent.ipynb notebook\" width=\"600\"/>\n",
    "\n",
    "- Training the agent enables it to solve the environment in 521 episodes (well below the 1800 episodes indicated for the challenge), with a constraint on the number of time steps per episode of 250 (instead of the 300 imposed by the environment).\n",
    "- We can see that the agent solves the environment more and more quickly as training progresses, as indicated by the average time-step curve for successfully solving the environment (i.e. reaching the score of 13).\n",
    "\n",
    "The model weights associated with the agent are saved in the file `model_weights_275_solved.pth` and can be used to test the agent in the environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8656055",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c971fc",
   "metadata": {},
   "source": [
    "We obtain the following results running the evaluation part of the `Training DQN Agent.ipynb` notebook:\n",
    "\n",
    "<img src=\"./images/model_weights_275_solved evaluation stats.png\" alt=\"Stats from training with Training DQN Agent.ipynb notebook\" width=\"600\"/>\n",
    "\n",
    "We note that the objectives set were achieved, even with tighter constraints on the number of \n",
    "episodes to solve the problem, the maximum number of time steps available per episode, as well as the stability of the rolling average over 100 episodes. We also note that even after 100 episodes, the rolling average is stable. In the end, only one successful episode fails to meet the constraint of the maximum number of time steps imposed.\n",
    "\n",
    "On the other hand, a non-negligible proportion of episodes (~29%) are not solved, but these are offset by the high scores of the successful ones.\n",
    "\n",
    "A video of the agent's interaction with the environment is available on [YouTube](https://youtu.be/G3rj4Yoc8bQ).\n",
    "\n",
    "<img src=\"./images/youtube video.png\" alt=\"Youtube video\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7844e1a8",
   "metadata": {},
   "source": [
    "## Ideas for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1e6ad",
   "metadata": {},
   "source": [
    "Some ideas for improving the DQN method:\n",
    "- carry out research into optimizing the values of hyperparameters related to the neural network structure (number of layers, layer size, etc.) and the DQN algorithm (learning rate, batch size, discount factor $\\gamma$, exploration rate $\\epsilon$, number of time steps per episode, etc);\n",
    "- study the importance of network initialization;\n",
    "- use of other DQN extensions, such as Double DQN, Dueling DQN, Prioritized Experience Replay, etc;\n",
    "- study the evolution of negative rewards:\n",
    "    - adding significant pmlus penalties?\n",
    "    - how to increase potential gains in the medium term train actions producing negative rewards in the short term (by playing on $\\gamma$ and the length of a sequence);\n",
    "- go into learning mode with the pixel state space to see if the agent can perform better, and what cost this entails compared to learning with the “discrete” state space.\n",
    "\n",
    "Other ideas for improvements:\n",
    "- study the failure rate of episodes when the average reward reaches the target;\n",
    "- How many training episodes are needed to increase the rate of successful episodes in evaluation?\n",
    "- what is the maximum average score that can be achieved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a908406",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

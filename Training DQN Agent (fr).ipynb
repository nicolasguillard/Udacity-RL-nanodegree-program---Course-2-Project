{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent DQN avec reprise d'expérience et estimation de la fonction d'action cible fixée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce carnet Jupyter, nous avons implémenté un agent DQN avec une mémoire d'expérience et une cible fixe. Il est entrainé pour résoudre l'environnement \"Banana\" de Unity. Nous avons utilisé la bibliothèque PyTorch pour construire le réseau de neurones et gérer l'entraînement. L'agent DQN utilise une mémoire d'expérience pour stocker les transitions et un réseau de neurones pour approximer la fonction Q. Une cible fixe est mise à jour périodiquement pour stabiliser l'apprentissage.\n",
    "\n",
    "Les contraintes de l'environnement sont les suivantes :\n",
    "- L'agent doit collecter des bananes jaunes tout en évitant les bananes bleues.\n",
    "- L'agent reçoit une récompense positive pour chaque banane jaune collectée et une récompense négative pour chaque banane bleue collectée.\n",
    "- L'agent doit apprendre à naviguer dans l'environnement pour maximiser sa récompense totale.\n",
    "- L'agent dispose de 300 pas de temps pour collecter le maximum de bananes jaunes, ensuite l'épisode d'interactions est indiqué comme terminé par l'environnement.\n",
    "\n",
    "Un épisode est considéré comme réussi si l'agent collecte au moins 13 bananes.\n",
    "\n",
    "L'objectif est d'atteindre une moyenne glissante sur 100 épisodes de score supérieure ou égale à 13. Un autre défi (optionnel) consiste à résoudre l'environnement \"Banana\" en moins de 1800 épisodes. Dans ce carnet, nous avons ajouté une autre contrainte en considérant qu'il fallait que la moyenne glissante sur 100 épisodes soit conservée sur plusieurs épisodes pour considérer l'atteinte de l'objectif comme stable. Par ailleurs, nous avons ajouté la possibilité d'indiquer des contraintes plus fortes sur le nombre maximum de pas de temps disponibles durant un épisode, le nombre d'épisodes pour résoudre l'environnement.\n",
    "\n",
    "Les définitions des composantes de l'agent DQN (classe `DQNAgentExpReplay`) sont incluses dans des fichiers séparés pour une meilleure lisibilité et modularité, et appelées dans ce carnet.\n",
    "\n",
    "Le carnet est divisé en plusieurs sections :\n",
    "1. **Importation des bibliothèques** : Nous importons les bibliothèques nécessaires pour l'entraînement de l'agent DQN.\n",
    "2. **Initialisation des paramètres et définitions des fonctions globales** : nous définitions les paramètres globaux et les fonctions utiles dans ce carnet.\n",
    "3. **Entraînement de l'agent** : Nous définissons et utilisons la fonction `train_agent` qui entraîne l'agent DQN sur l'environnement \"Banana\". Cette étape peut être passée si l'agent a déjà été entraîné, disposant alors d'un fichier des poids du modèle sous-jacent, et que nous souhaitons seulement évaluer ses performances.\n",
    "4. **Évaluation de l'agent** : Nous évaluons les performances de l'agent sur l'environnement \"Banana\".\n",
    "\n",
    "\n",
    "> Note : Certaines parties de ce carnet et les codes inclus sont en partie inspirés des codes fournis par Udacity dans le cadre du projet de la formation \"Deep Reinforcement Learning Nanodegree\". Nous avons également utilisé des ressources en ligne pour nous aider à construire l'agent DQN et à gérer l'entraînement. Nous avons veillé à respecter les bonnes pratiques de codage et à commenter le code pour faciliter la compréhension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in and 3rd party modules\n",
    "from typing import Callable\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "# Custom modules\n",
    "from dqn_agent__expreplay_fixedqtarget import DQNAgentExpReplayFixedQTarget as DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions et initialisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons un typage particulier pour les statistiques des épisodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode_Stats = namedtuple(\"Experience\", field_names=[\"i_episode\", \"steps_to_resolution\", \"score\", \"is_solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons une fonction d'affichage des courbes des statistiques et des scores obtenus durant l'apprentissage ou l'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_stats(\n",
    "        episode_stats: list[Episode_Stats],\n",
    "        mean_scores: list[float],\n",
    "        shifted_score_avgs: list[float],\n",
    "        solution_threshold: int,\n",
    "        scores_window_length: int,\n",
    "        title: str=\"\") -> None:\n",
    "    \"\"\"Display the training statistics in several plots.\n",
    "    \n",
    "    Args:\n",
    "        episode_stats (list[Episode_Stats]): List of episode statistics.\n",
    "        mean_scores (list[float]): List of mean scores.\n",
    "        shifted_score_avgs (list[float]): List of shifted score averages.\n",
    "        solution_threshold (int): Threshold for a solution.\n",
    "        scores_window_length (int): Length of the window score average.\n",
    "        title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    scores = [episode.score for episode in episode_stats]\n",
    "    ax.plot(np.arange(1, len(scores)+1), scores, color='b', label='Scores')\n",
    "    # Plot the mean scores\n",
    "    ax.plot(\n",
    "        np.arange(1, len(mean_scores)+1),\n",
    "        mean_scores,\n",
    "        color='y',\n",
    "        linestyle='--',\n",
    "        label='Score average from start'\n",
    "        )\n",
    "    # Plot the shifted score averages\n",
    "    ax.plot(\n",
    "        np.arange(scores_window_length, scores_window_length + len(shifted_score_avgs)),\n",
    "        shifted_score_avgs,\n",
    "        color='r',\n",
    "        label=f\"{scores_window_length} episodes shifted average\"\n",
    "        )\n",
    "    ax.set_ylabel('Score')\n",
    "    \n",
    "    # Plot the episode length (count of steps) when successful\n",
    "    x_scatter = []\n",
    "    y_scatter = []\n",
    "    max_steps_to_resolution = 0\n",
    "    for episode in episode_stats:\n",
    "        if episode.is_solution:\n",
    "            x_scatter.append(episode.i_episode)\n",
    "            y_scatter.append(episode.steps_to_resolution)\n",
    "            max_steps_to_resolution = max(max_steps_to_resolution, episode.steps_to_resolution)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.scatter(\n",
    "        x_scatter,\n",
    "        y_scatter,\n",
    "        s=2,\n",
    "        color='g',\n",
    "        hatch=\"x\",\n",
    "        label='Steps to success'\n",
    "    )\n",
    "    ax2.set_ylim(0, int(1.1 * max_steps_to_resolution))\n",
    "    ax2.set_ylabel('Steps')\n",
    "\n",
    "    # Plot the solution threshold line\n",
    "    ax.axhline(y=solution_threshold, color='k', linestyle='--', label='Solution threshold')\n",
    "    \n",
    "    # Display the figure with legend\n",
    "    ax.set_xlabel('Episode #')\n",
    "    plt.title(title)\n",
    "    fig.legend()\n",
    "    plt.show(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons une fonction permettant de lancer un environnement Unity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_unity_env(file_name: str, train_mode: bool = False) -> UnityEnvironment:\n",
    "    \"\"\"Launch the Unity environment.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the Unity environment executable.\n",
    "        train_mode (bool): Whether to launch in training mode or not.\n",
    "    \"\"\"\n",
    "    env = UnityEnvironment(file_name)\n",
    "\n",
    "    # Get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "\n",
    "    # Number of agents in the environment\n",
    "    print('Number of agents:', len(env_info.agents))\n",
    "    \n",
    "    return env, brain_name, brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser un GPU si disponible, sinon nous utiliserons le CPU. `mps` est le GPU d'Apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "print(f\"Availabe device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paramètres globaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons si nécessaire une graine aléatoire pour la reproductibilité des résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 71 # Set the random seed for reproducibility\n",
    "\n",
    "if SEED is not None:\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons les hyperparamètres utiles à la définition de l'agent, son entraînement et son évaluation. \n",
    "\n",
    "Par ailleurs, nous renforçons les contraintes sur le nombre maximum de pas de temps disponibles durant un épisode avec `max_timesteps`, sur le nombre maximum d'épisodes pour résoudre l'environnement avec `n_episodes_train`, et ajoutons la contrainte de stabilité pour considérer l'atteinte de l'objectif comme stable avec `window_stability`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent neural netwotk settings\n",
    "model_parameters = {\n",
    "    \"fc1_units\": 64,\n",
    "    \"fc2_units\": 64,\n",
    "}\n",
    "\n",
    "# Training hyperparameters\n",
    "## Epsilon scheduler parameters\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.995\n",
    "\n",
    "n_episodes_train = 1500         # Maximum number of training episodes to solve the environment\n",
    "max_timesteps = 250             # Max number of timesteps per episode. After 300 timestep the episode is done, as observed in the \"Take Random Actions in the Environment\" test\n",
    "window_stability = 5            # Number of episodes to consider the agent learning as stable\n",
    "avg_window_length_scores = 100  # Project success constraint\n",
    "solution_threshold = 13         # Project success constraint\n",
    "print_stats_each_n_episode = 25\n",
    "\n",
    "weight_filename_prefix = \"model_weights\"\n",
    "\n",
    "# Evaluation hyperparameters\n",
    "n_episodes_test = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous appellons et initialisons l'environnement Unity, en reprenant les indications fournies dans le carnet `Navigation.ipynb`, et nous récupérons les informations utile pour les interactions avec l'environnement. Cette initialisation peut servir dans ce carnet à la fois pour l'entraînement et l'évaluation de l'agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, brain_name, brain = launch_unity_env(\"Banana.app\", train_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement de l'agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition de la fonction d'entrainement d'un agent DQN sur un environnement Unity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_agent(\n",
    "        agent: DQNAgent,\n",
    "        env: UnityEnvironment,\n",
    "        brain_name: str,\n",
    "        eps_scheduler: callable,\n",
    "        eps_start: float = 1.0,\n",
    "        n_max_episodes: int = 2000,\n",
    "        solution_threshold: int = 13,\n",
    "        avg_window_length_scores: int = 100,\n",
    "        print_stats_each_n_episode: int = 25,\n",
    "        window_stability: int = 1,\n",
    "        max_timesteps: int = 300,\n",
    "        weight_filename_prefix: str = \"checkpoint\", # None to not save weights\n",
    "        ) -> tuple[list[Episode_Stats], list[float], list[float]]:\n",
    "    \"\"\"Train the DQN agent in the environment.\n",
    "    \n",
    "    Args:\n",
    "        agent (DQNAgentExpReplay): The DQN agent to train.\n",
    "        env (UnityEnvironment): The Unity environment.\n",
    "        brain_name (str): The name of the brain in the environment.\n",
    "        eps_scheduler (function): Function to schedule epsilon.\n",
    "        eps_start (float): Initial epsilon value.\n",
    "        n_max_episodes (int): Number of episodes to train the agent.\n",
    "        solution_threshold (int): Threshold for a solution.\n",
    "        avg_window_length_scores (int): Length of the window score average.\n",
    "        print_stats_each_n_episode (int): Frequency of printing stats.\n",
    "        window_stability (int): Window length for stability check.\n",
    "        max_timesteps (int): Maximum number of timesteps per episode.\n",
    "        weight_filename_prefix (str): Prefix for saving weights. If None, weights are not saved. Filename will be suffixed with \"_solved\" or \"_last_episode\", the weights are saved when the environment is solved or at the end of training, respectively.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - stats_episodes (list[Episode_Stats]): List of episode statistics.\n",
    "            - score_avgs (list[float]): List of mean scores.\n",
    "            - shift_score_avgs (list[float]): List of shifted score averages.\n",
    "    \"\"\"\n",
    "    # List of stats for each episode\n",
    "    scores = []\n",
    "    score_avgs = []\n",
    "    shifted_score_avgs = []\n",
    "    stats_episodes = []\n",
    "\n",
    "    eps = eps_start             # Epsilon for exploration\n",
    "    solved = False              # Flag to ind6icate if the environment is solved in the episode\n",
    "    last_solved = 0             # Last episode indice where the environment was solved\n",
    "    solved_episode_count = 0    # Count of episodes where the environment was solved\n",
    "\n",
    "    agent.train()\n",
    "    for i_episode in trange(1, n_max_episodes+1, desc=\"Training\", unit=\"episode\", leave=False):\n",
    "        # Reset the environment\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        # Initial state\n",
    "        state = env_info.vector_observations[0]\n",
    "\n",
    "        score = 0               # episode score\n",
    "        actions_of_episode = [] # list of actions taken during the episode\n",
    "        steps_to_resolution = 0\n",
    "        # Loop for each episode\n",
    "        for i_step in range(1, max_timesteps+1):\n",
    "            # Agent chooses action\n",
    "            action = agent.act(state, eps=eps)\n",
    "            actions_of_episode.append(action)\n",
    "            \n",
    "            # Apply action to environment and get environment evolution as\n",
    "            # experience : next state, reward and done\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0] \n",
    "            \n",
    "            score += reward\n",
    "            if score >= solution_threshold and steps_to_resolution == 0:\n",
    "                steps_to_resolution = i_step\n",
    "            \n",
    "            # Agent learns from the experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state                \n",
    "            \n",
    "            # Check if episode is done (a.k.a terminal state)\n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        # Save episode stats\n",
    "        stats_episodes.append(\n",
    "            Episode_Stats(i_episode, steps_to_resolution, score, score >= solution_threshold)\n",
    "            )\n",
    "        scores.append(score)\n",
    "        score_avgs.append(np.mean(scores))\n",
    "        \n",
    "        if score >= solution_threshold:\n",
    "            last_solved = i_episode\n",
    "            solved_episode_count += 1\n",
    "\n",
    "        # Process shifted score average when enough scores\n",
    "        if i_episode >= avg_window_length_scores:\n",
    "            shifted_score_avgs.append(np.mean(scores[-avg_window_length_scores:]))\n",
    "\n",
    "            # Print stats regularly\n",
    "            if i_episode % print_stats_each_n_episode == 0:\n",
    "                print(f'\\rEpisode {i_episode} | Mean scores {score_avgs[-1]:.2f}' + \\\n",
    "                        f'| {avg_window_length_scores} shift score average: {shifted_score_avgs[-1]:.2f}' + \\\n",
    "                        f'| Environment solved {solved_episode_count} time(s).',\n",
    "                        end=\" \")\n",
    "                if last_solved > 0:\n",
    "                    print(f'Last solution at episode {last_solved}th with score {stats_episodes[-1].score}.', end=\"\")\n",
    "\n",
    "            # Check the stability of the solution\n",
    "            if np.mean(shifted_score_avgs[-window_stability:]) >= solution_threshold:\n",
    "                print(f'\\n>> Environment solved in {i_episode} episodes!\\tAverage Score: {shifted_score_avgs[-1]:.2f} stable')\n",
    "                if weight_filename_prefix is not None:\n",
    "                    filename = f\"{weight_filename_prefix}_solved.pth\"\n",
    "                    agent.save(filename)\n",
    "                    print(f\"Model weights saved to {filename}\")\n",
    "                solved = True\n",
    "                break\n",
    "\n",
    "        # Update epsilon\n",
    "        eps = eps_scheduler(i_step, eps)\n",
    "\n",
    "    # Save the last episode if environment not solved\n",
    "    if not solved:\n",
    "        print(f'\\n>> Environment not solved after {i_episode} episodes!\\tAverage Score: {shifted_score_avgs[-1]:.2f} (perhaps > {solution_threshold} but not stable yet)')\n",
    "        if weight_filename_prefix is not None:\n",
    "            filename = f\"{weight_filename_prefix}_last_episode.pth\"\n",
    "            agent.save(filename)\n",
    "            print(f\"\\nModel weights saved to {filename}\")\n",
    "\n",
    "    return stats_episodes, score_avgs, shifted_score_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons un agent en indiquant les paramètres d'initialisation, et nous l'entraînons sur l'environnement \"Banana\" en utilisant la fonction `train_agent`. A noter la création de `eps_scheduler` qui doit respecter une certaine interface (deux paramètres d'entrée `i_episode` et `eps`) pour être compatible avec la fonction d'entraînement de l'agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the agent\n",
    "agent = DQNAgent (\n",
    "    state_size=brain.vector_observation_space_size,\n",
    "    action_size=brain.vector_action_space_size,\n",
    "    model_parameters=model_parameters,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# eps_scheduler : function to modify epsilon\n",
    "# i_episode (based on 1) is the number of steps\n",
    "# eps is the current epsilon\n",
    "eps_scheduler = lambda i_episode, eps, : max(eps_end, eps_decay*eps)\n",
    "\n",
    "# Train the agent\n",
    "stats_episodes, score_avgs, shifted_score_avgs = train_dqn_agent(\n",
    "    agent,\n",
    "    env,\n",
    "    brain_name,\n",
    "    eps_scheduler,\n",
    "    eps_start=1.0,\n",
    "    n_max_episodes=n_episodes_train,\n",
    "    avg_window_length_scores=avg_window_length_scores,\n",
    "    solution_threshold=solution_threshold,\n",
    "    window_stability=window_stability,\n",
    "    print_stats_each_n_episode=print_stats_each_n_episode,\n",
    "    weight_filename_prefix=weight_filename_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affichons les courbes des stastistiques et des scores obtenus durant l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_stats(\n",
    "    stats_episodes, score_avgs, shifted_score_avgs, solution_threshold, avg_window_length_scores,\n",
    "    \"Training : scores and score averages\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation de l'agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous évaluons les performances de l'agent sur l'environnement \"Banana\" en utilisant la fonction. Nous affichons les courbes d'apprentissage et les scores obtenus durant le test.\n",
    "\n",
    "L'évaluation peut porter sur l'agent qui vient d'être entrainé, ou sur un agent dont les poids sont disponibles dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_filename = \"model_weights_solved.pth\" # Prefix for saving weights\n",
    "reload = True # Set to True to reload the last training weights\n",
    "if reload:\n",
    "    agent = DQNAgent(\n",
    "        state_size=brain.vector_observation_space_size,\n",
    "        action_size=brain.vector_action_space_size,\n",
    "        model_parameters=model_parameters,\n",
    "        device=device\n",
    "        )\n",
    "    agent.load(weight_filename)\n",
    "agent.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette étape permet de forcer l'affichage de la fenêtre de l'environnement Unity, afin de préparer les éventuels enregistrements d'écran pour des vidéos de démonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env is None:\n",
    "    env, brain_name, brain = launch_unity_env(\"Banana.app\", train_mode=False)\n",
    "else:\n",
    "    env_info = env.reset(train_mode=False)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons une fonctions d'évaluation de l'agent sur un environnement Unity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dqn_agent(\n",
    "        agent: DQNAgent,\n",
    "        env: UnityEnvironment,\n",
    "        brain_name: str,\n",
    "        n_episodes_test: int = 100,\n",
    "        solution_threshold: int = 13,\n",
    "        avg_window_length_scores: int = 100,\n",
    "        window_stability: int = 1\n",
    "    ) -> tuple[list[Episode_Stats], list[float], list[float]]:\n",
    "    \"\"\"Evaluate the DQN agent in the environment.\n",
    "    \n",
    "    Args:\n",
    "        agent (DQNAgent): The DQN agent to evaluate.\n",
    "        env (UnityEnvironment): The Unity environment.\n",
    "        brain_name (str): The name of the brain in the environment.\n",
    "        n_episodes_test (int): Number of episodes to test the agent.\n",
    "        solution_threshold (int): Threshold for a solution.\n",
    "        avg_window_length_scores (int): Length of the window score average.\n",
    "        window_stability (int): Window length for stability check.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - stats_episodes (list[Episode_Stats]): List of episode statistics.\n",
    "            - score_avgs (list[float]): List of mean scores.\n",
    "            - shift_score_avgs (list[float]): List of shifted score averages.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    stats_episodes = []\n",
    "    score_avgs = []\n",
    "    shifted_score_avgs = []\n",
    "\n",
    "    score_avg = 0\n",
    "    for i_episode in trange(1, n_episodes_test+1):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        \n",
    "        score = 0    \n",
    "        done = False\n",
    "        steps_to_resolution = 0\n",
    "        while not done:\n",
    "            # Agent chooses action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Apply action to environment\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            \n",
    "            # Get environment evolution as experience : next state, reward and done\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            state = next_state\n",
    "\n",
    "            # Update the score\n",
    "            score += reward                               \n",
    "            steps_to_resolution += 1\n",
    "\n",
    "            print(f\"\\rEpisode #{i_episode} : Score = {int(score)} in {steps_to_resolution} steps| Score avg = {score_avg:.2f}\", end=\"\")\n",
    "\n",
    "        # Save episode stats\n",
    "        stats_episodes.append(\n",
    "                Episode_Stats(i_episode, steps_to_resolution, score, score >= solution_threshold)\n",
    "                )\n",
    "        scores.append(score)\n",
    "        score_avg = np.mean(scores)\n",
    "        score_avgs.append(score_avg)\n",
    "        \n",
    "        # Process shifted score average when enough scores\n",
    "        if i_episode >= avg_window_length_scores:\n",
    "            shifted_score_avgs.append(np.mean(scores[-avg_window_length_scores:]))\n",
    "            # Check the shifted score average stability\n",
    "            if np.mean(shifted_score_avgs[-window_stability:]) >= solution_threshold:\n",
    "                print(f\"\\rEpisode #{i_episode} : Score = {int(score)} in {steps_to_resolution} steps| Score avg = {score_avgs[-1]:.2f}\", end=\"\")\n",
    "                break\n",
    "\n",
    "    return stats_episodes, score_avgs, shifted_score_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous lançons l'évaluation de l'agent sur l'environnement \"Banana\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_stats_episodes, eval_score_avgs, eval_shifted_score_avgs = eval_dqn_agent(\n",
    "    agent,\n",
    "    env,\n",
    "    brain_name,\n",
    "    n_episodes_test=n_episodes_test,\n",
    "    solution_threshold=solution_threshold,\n",
    "    avg_window_length_scores=avg_window_length_scores,\n",
    "    window_stability=window_stability\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis nous affichons les courbes de scores obtenus durant l'évaluation, constatant que les objectifs fixés ont été atteints, même avec une contrainte plus forte sur le nombre maximum de pas de temps disponibles durant un épisode et la stabilité de la moyenne glissante sur 100 épisodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_stats(\n",
    "    eval_stats_episodes, eval_score_avgs, eval_shifted_score_avgs, solution_threshold, avg_window_length_scores,\n",
    "    \"Testing : scores and score averages\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du carnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous fermons l'environnement Unity ouvert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
